# app/db/database.py
from __future__ import annotations

import logging
import time

from contextlib import asynccontextmanager
from typing import AsyncGenerator, Optional

from sqlalchemy import event, text, inspect
from sqlalchemy.engine import Engine
from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.pool import NullPool, AsyncAdaptedQueuePool

from app.config import settings

logger = logging.getLogger(__name__)


# =========================
# Declarative Base
# =========================
class Base(DeclarativeBase):
    pass


# =========================
# ĞŸÑƒĞ» ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹
# =========================
is_sqlite = settings.db_dsn.startswith("sqlite")
if is_sqlite:
    poolclass = NullPool
    pool_kwargs: dict = {}
    connect_args: dict = {}  # Ğ´Ğ»Ñ sqlite Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾
else:
    poolclass = AsyncAdaptedQueuePool
    pool_kwargs = {
        "pool_size": 20,
        "max_overflow": 30,
        "pool_timeout": 30,
        "pool_recycle": 3600,
        "pool_pre_ping": True,
        "pool_reset_on_return": "rollback",
    }
    # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ server_settings Ğ´Ğ»Ñ Postgres (asyncpg)
    connect_args = {
        "server_settings": {
            "application_name": "supportbot",
            "statement_timeout": "60000",                     # 60s
            "idle_in_transaction_session_timeout": "300000",  # 5m
        },
        "command_timeout": 60,
        "timeout": 10,
    }

# =========================
# Engine
# =========================

# ĞĞ±Ñ‰Ğ¸Ğµ execution_options
execution_options = {"compiled_cache_size": 500}
# Ğ”Ğ»Ñ Postgres/Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¡Ğ£Ğ‘Ğ” Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ READ COMMITTED,
# Ğ´Ğ»Ñ sqlite â€” Ğ½ĞµĞ»ÑŒĞ·Ñ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½Ğµ Ñ‚Ñ€Ğ¾Ğ³Ğ°ĞµĞ¼.
if not is_sqlite:
    execution_options["isolation_level"] = "READ COMMITTED"

engine: AsyncEngine = create_async_engine(
    settings.db_dsn,
    echo=settings.is_dev,  # Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ Ğ² dev
    future=True,
    poolclass=poolclass,
    execution_options=execution_options,
    connect_args=connect_args,
    **pool_kwargs,
)

# =========================
# Session factory
# =========================
AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False,   # Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑÑÑ€Ğ¿Ñ€Ğ¸Ğ·Ğ¾Ğ²
    autocommit=False,
)

# ============================================================================
# ADVANCED SESSION MANAGER WITH READ REPLICAS
# ============================================================================

class DatabaseManager:
    """ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ‘Ğ” Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ¸ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""

    def __init__(self):
        self.engine = engine
        self.read_replica_engine: Optional[AsyncEngine] = None

        if hasattr(settings, 'DATABASE_READ_REPLICA_URL') and settings.DATABASE_READ_REPLICA_URL:
            self.read_replica_engine = create_async_engine(
                settings.DATABASE_READ_REPLICA_URL,
                poolclass=poolclass,
                pool_size=30,  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ»Ñ read Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
                max_overflow=50,
                pool_pre_ping=True,
                echo=False,
            )

    @asynccontextmanager
    async def session(self, read_only: bool = False):
        target_engine = self.read_replica_engine if (read_only and self.read_replica_engine) else self.engine

        async_session = async_sessionmaker(
            bind=target_engine,
            class_=AsyncSession,
            expire_on_commit=False,
            autoflush=False,
        )

        async with async_session() as session:
            try:
                yield session
                if not read_only:
                    await session.commit()
            except Exception:
                await session.rollback()
                raise

    async def health_check(self) -> dict:
        pool = self.engine.pool

        try:
            async with AsyncSessionLocal() as session:
                start = time.time()
                await session.execute(text("SELECT 1"))
                latency = (time.time() - start) * 1000
            status = "healthy"
        except Exception as e:
            logger.error(f"âŒ Database health check failed: {e}")
            status = "unhealthy"
            latency = None

        return {
            "status": status,
            "latency_ms": round(latency, 2) if latency else None,
            "pool": _collect_health_pool_metrics(pool),
        }

db_manager = DatabaseManager()

# =========================
# DI / dependency
# =========================
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    """Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¸ (FastAPI/Aiogram)."""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            # ĞšĞ¾Ğ¼Ğ¼Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑĞ»Ğ¾Ğµ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ·Ğ´ĞµÑÑŒ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ĞµĞ¼
            # (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ Â«ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Â»).
            pass


# ============================================================================
# INITIALIZATION AND CLEANUP
# ============================================================================

async def init_db():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ‘Ğ” Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸"""
    logger.info("ğŸš€ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    if not settings.db_dsn.startswith("sqlite"):
        logger.info("ğŸ“Š Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸...")

        async with engine.begin() as conn:
            indexes = [
                ("users", "CREATE INDEX IF NOT EXISTS idx_users_telegram_id ON users(telegram_id)")
            ]

            for table_name, index_sql in indexes:
                table_exists = await conn.run_sync(lambda sync_conn: inspect(sync_conn).has_table(table_name))

                if not table_exists:
                    logger.debug(
                        "ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° %s: Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° %s Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚",
                        index_sql,
                        table_name,
                    )
                    continue

                try:
                    await conn.execute(text(index_sql))
                except Exception as e:
                    logger.debug("Index creation skipped for %s: %s", table_name, e)

    logger.info("âœ… Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°")

    health = await db_manager.health_check()
    logger.info(f"ğŸ“Š Database health: {health}")

async def close_db():
    """ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ²ÑĞµÑ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹"""
    logger.info("ğŸ”„ Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ñ Ğ‘Ğ”...")

    await engine.dispose()

    if db_manager.read_replica_engine:
        await db_manager.read_replica_engine.dispose()

    logger.info("âœ… Ğ’ÑĞµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹")

# ============================================================================
# CONNECTION POOL METRICS (Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°)
# ============================================================================

def _pool_counters(pool):
    """Return basic pool counters or ``None`` when unsupported."""

    required_methods = ("size", "checkedin", "checkedout", "overflow")

    for method_name in required_methods:
        method = getattr(pool, method_name, None)
        if method is None or not callable(method):
            return None

    size = pool.size()
    checked_in = pool.checkedin()
    checked_out = pool.checkedout()
    overflow = pool.overflow()

    total_connections = size + overflow

    return {
        "size": size,
        "checked_in": checked_in,
        "checked_out": checked_out,
        "overflow": overflow,
        "total_connections": total_connections,
        "utilization_percent": (checked_out / total_connections * 100) if total_connections else 0.0,
    }


def _collect_health_pool_metrics(pool) -> dict:
    counters = _pool_counters(pool)

    if counters is None:
        return {
            "metrics_available": False,
            "size": 0,
            "checked_in": 0,
            "checked_out": 0,
            "overflow": 0,
            "total_connections": 0,
            "utilization": "0.0%",
        }

    return {
        "metrics_available": True,
        "size": counters["size"],
        "checked_in": counters["checked_in"],
        "checked_out": counters["checked_out"],
        "overflow": counters["overflow"],
        "total_connections": counters["total_connections"],
        "utilization": f"{counters['utilization_percent']:.1f}%",
    }


async def get_pool_metrics() -> dict:
    """Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿ÑƒĞ»Ğ° Ğ´Ğ»Ñ Prometheus/Grafana"""
    pool = engine.pool

    counters = _pool_counters(pool)

    if counters is None:
        return {
            "metrics_available": False,
            "pool_size": 0,
            "checked_in_connections": 0,
            "checked_out_connections": 0,
            "overflow_connections": 0,
            "total_connections": 0,
            "max_possible_connections": 0,
            "pool_utilization_percent": 0.0,
        }

    return {
        "metrics_available": True,
        "pool_size": counters["size"],
        "checked_in_connections": counters["checked_in"],
        "checked_out_connections": counters["checked_out"],
        "overflow_connections": counters["overflow"],
        "total_connections": counters["total_connections"],
        "max_possible_connections": counters["total_connections"] + (getattr(pool, "_max_overflow", 0) or 0),
        "pool_utilization_percent": round(counters["utilization_percent"], 2),
    }

# =========================
# Debug: Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
# =========================
if settings.is_dev:
    # Ğ’ dev Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ "ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ°".
    from sqlalchemy import event

    @event.listens_for(Engine, "before_cursor_execute")
    def _before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        conn.info.setdefault("query_start_time", []).append(time.perf_counter())

    @event.listens_for(Engine, "after_cursor_execute")
    def _after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        start_list = conn.info.get("query_start_time")
        if not start_list:
            return
        total = time.perf_counter() - start_list.pop(-1)
        # ĞŸĞ¾Ñ€Ğ¾Ğ³ â€” 100ms. Ğ’ÑÑ‘, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ»ÑŒÑˆĞµ, Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğº slow.
        if total > 0.1:
            logger.warning("ğŸŒ Slow query (%.3fs): %s", total, statement[:120])
        else:
            logger.debug("âš¡ Query in %.3fs", total)
